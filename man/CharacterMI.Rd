% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataset_information.R
\name{CharacterMI}
\alias{CharacterMI}
\alias{CharacterEntropy}
\alias{CharacterEntropies}
\alias{ApportionAmbiguity}
\alias{JointCharacterEntropy}
\alias{JointCharacterEntropies}
\alias{MutualCharacterInformation}
\alias{MutCharInfo}
\title{Mutual information between characters}
\usage{
CharacterEntropy(char, ignore = character(0))

CharacterEntropies(chars, ignore = character(0))

ApportionAmbiguity(char, ignore = character(0))

JointCharacterEntropy(
  char1,
  char2,
  ignore = character(0),
  ignore1 = NULL,
  ignore2 = NULL,
  states1 = NULL,
  states2 = NULL,
  tokens1 = NULL,
  tokens2 = NULL
)

JointCharacterEntropies(characters, ignore = character(0))

MutualCharacterInformation(characters, ignore = character(0))

MutCharInfo(characters, ignore = character(0))
}
\arguments{
\item{char, char1, char2}{Character vector listing character tokens.
For handling of ambiguous tokens, see section 'Ambiguity'.}

\item{ignore}{Character vector listing tokens to treat as ambiguous,
besides \verb{?} and \code{-}.}
}
\description{
Calculate the entropy, joint entropy and mutual information between
characters in a morphological dataset,
distinguishing 'background' mutual information due to
shared ancestry and random chance from functionally significant correlations.
}
\details{
\code{CharacterEntropy()} and \code{CharacterEntropies()} calculate the entropy of
individual characters, using \code{ApportionAmiguity()} to assign frequencies
to ambiguous tokens.

\code{JointCharacterEntropy()} and \code{JointCharacterEntropies()} calculate the
joint entropy of pairs of characters.

\code{MutualCharacterInformation()} calculates the mutual information between
each pair of characters: i.e. the extent to which the state of one character
can be guessed based on the state of the other.
}
\section{Ambiguity}{

Ambiguous characters should be specified as a string containing all possible
characters states (and, optionally, parentheses): e.g. \verb{[01]}.
\verb{?} and \code{-} will be interpreted as 'any possible value'.
Each token is assigned a probability according to its number of
\emph{non-ambiguous} occurrences in the character.
Example: in \verb{000000 1111 [01] [02] ?}, \verb{[01]} and \verb{?} will be taken to have
a 6/10 chance of being token \code{0};
\verb{[02]} will be treated as having a 100\% chance of being state \code{0}.
}

\section{#TODO Notes}{

MI = MIb (background, = random noise + shared ancestry) +
MIsf (structure and function)
MIp approximates MIsf
High mean MIb implies strong phylogenetic signal
}

\examples{
data('Lobo', package = 'TreeTools')
dat <- PhyDatToMatrix(Lobo.phy)[, 1:6]

CharacterEntropies(dat)
apply(dat, 2, CharacterEntropy)
JointCharacterEntropies(dat)
MutualCharacterInformation(dat)


char <- c(rep('0', 6), rep('1', 4), '[01]', '[02]', '?')
ShareAmbiguity(char)
ShareAmbiguity(char, ignore = '[02]')
CharacterEntropy(char)
}
\references{
\insertRef{Dunn2008}{TreeTools}
}
\author{
Martin R. Smith (\href{mailto:martin.smith@durham.ac.uk}{martin.smith@durham.ac.uk})
}
